{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vaex\n",
    "vaex.settings.display.float_format = '%.8f'\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded April_2024.csv into Vaex DataFrame with key: April_2024\n",
      "Loaded August_2024.csv into Vaex DataFrame with key: August_2024\n",
      "Loaded December_2023.csv into Vaex DataFrame with key: December_2023\n",
      "Loaded Feb_2024.csv into Vaex DataFrame with key: Feb_2024\n",
      "Loaded Jan_2024.csv into Vaex DataFrame with key: Jan_2024\n",
      "Loaded July_2024.csv into Vaex DataFrame with key: July_2024\n",
      "Loaded June_2024.csv into Vaex DataFrame with key: June_2024\n",
      "Loaded March_2024.csv into Vaex DataFrame with key: March_2024\n",
      "Loaded May_2024.csv into Vaex DataFrame with key: May_2024\n",
      "Loaded November_2023.csv into Vaex DataFrame with key: November_2023\n",
      "Loaded October_2023.csv into Vaex DataFrame with key: October_2023\n",
      "Loaded October_2024.csv into Vaex DataFrame with key: October_2024\n",
      "Loaded September_2024.csv into Vaex DataFrame with key: September_2024\n"
     ]
    }
   ],
   "source": [
    "folder_path = './cleaned_dataset'\n",
    "csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "\n",
    "# Dictionary to store dataframes\n",
    "dfs_dict = {}\n",
    "\n",
    "# Load each CSV into Vaex and store in the dictionary\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    key_name = file.replace(\".csv\", \"\")  # Remove the .csv extension for the key\n",
    "    dfs_dict[key_name] = vaex.open(file_path)\n",
    "    print(f\"Loaded {file} into Vaex DataFrame with key: {key_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mapping dictionary to reduce the string length\n",
    "\n",
    "route_names = dfs_dict['September_2024']['Route Name'].unique()\n",
    "# mapping dictionary\n",
    "route_names_map = {}\n",
    "\n",
    "for route_name in route_names:\n",
    "    if \"EXPRESS\" in route_name:\n",
    "        route_names_map[route_name] = route_name.split(' ', 1)[0] \n",
    "    elif \"TUNNEL\" in route_name:\n",
    "        continue\n",
    "    else:\n",
    "        route_names_map[route_name] = route_name.split(' ', 2)[-1]\n",
    "\n",
    "def route_name_map(df):\n",
    "    df['Route Name'] = df['Route Name'].apply(lambda x: route_names_map.get(x, x))\n",
    "    return df\n",
    "\n",
    "for i, (key, df) in enumerate(dfs_dict.items()):\n",
    "    dfs_dict[key] = route_name_map(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed missing values from April_2024 (1/13)\n",
      "Removed missing values from August_2024 (2/13)\n",
      "Removed missing values from December_2023 (3/13)\n",
      "Removed missing values from Feb_2024 (4/13)\n",
      "Removed missing values from Jan_2024 (5/13)\n",
      "Removed missing values from July_2024 (6/13)\n",
      "Removed missing values from June_2024 (7/13)\n",
      "Removed missing values from March_2024 (8/13)\n",
      "Removed missing values from May_2024 (9/13)\n",
      "Removed missing values from November_2023 (10/13)\n",
      "Removed missing values from October_2023 (11/13)\n",
      "Removed missing values from October_2024 (12/13)\n",
      "Removed missing values from September_2024 (13/13)\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows with missing values for each Vaex DataFrame in the dictionary\n",
    "for i, (key, df) in enumerate(dfs_dict.items()):\n",
    "    #print(f'Rows before removing missing values: {len(df):,}')\n",
    "    # Drop missing values (returns a new Vaex DataFrame)\n",
    "    dfs_dict[key] = df.dropna(column_names=['stop_lon', 'Stop Code'])\n",
    "    #print(f'Rows after removing missing values: {len(dfs_dict[key]):,}')\n",
    "    print(f\"Removed missing values from {key} ({i+1}/{len(dfs_dict)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized stop_sequence in April_2024 (1/13)\n",
      "Normalized stop_sequence in August_2024 (2/13)\n",
      "Normalized stop_sequence in December_2023 (3/13)\n",
      "Normalized stop_sequence in Feb_2024 (4/13)\n",
      "Normalized stop_sequence in Jan_2024 (5/13)\n",
      "Normalized stop_sequence in July_2024 (6/13)\n",
      "Normalized stop_sequence in June_2024 (7/13)\n",
      "Normalized stop_sequence in March_2024 (8/13)\n",
      "Normalized stop_sequence in May_2024 (9/13)\n",
      "Normalized stop_sequence in November_2023 (10/13)\n",
      "Normalized stop_sequence in October_2023 (11/13)\n",
      "Normalized stop_sequence in October_2024 (12/13)\n",
      "Normalized stop_sequence in September_2024 (13/13)\n"
     ]
    }
   ],
   "source": [
    "def stop_sequence_normalization(df):\n",
    "    df = df.to_pandas_df()\n",
    "    # Convert 'Business Day' to datetime format if not already\n",
    "    df['Business Day'] = pd.to_datetime(df['Business Day'])\n",
    "\n",
    "    # Normalize stop_sequence within each group\n",
    "    df['stop_sequence_normalized'] = df.groupby(['Business Day', 'Route Name', 'Direction', 'daily_order_trip_id'])['stop_sequence'].transform(\n",
    "        lambda x: np.round((x - 1) / (x.max() - 1),3) if x.nunique() > 1 else 0\n",
    "    )\n",
    "\n",
    "    df = vaex.from_pandas(df)\n",
    "    df['Business Day'] = df['Business Day'].astype('datetime64').dt.strftime('%Y-%m-%d')\n",
    "    #df = df.drop('stop_sequence')\n",
    "    return df\n",
    "\n",
    "for i, (key, df) in enumerate(dfs_dict.items()):\n",
    "    dfs_dict[key] = stop_sequence_normalization(df)\n",
    "    print(f\"Normalized stop_sequence in {key} ({i+1}/13)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized daily_order_trip_id in April_2024 (1/13)\n",
      "Normalized daily_order_trip_id in August_2024 (2/13)\n",
      "Normalized daily_order_trip_id in December_2023 (3/13)\n",
      "Normalized daily_order_trip_id in Feb_2024 (4/13)\n",
      "Normalized daily_order_trip_id in Jan_2024 (5/13)\n",
      "Normalized daily_order_trip_id in July_2024 (6/13)\n",
      "Normalized daily_order_trip_id in June_2024 (7/13)\n",
      "Normalized daily_order_trip_id in March_2024 (8/13)\n",
      "Normalized daily_order_trip_id in May_2024 (9/13)\n",
      "Normalized daily_order_trip_id in November_2023 (10/13)\n",
      "Normalized daily_order_trip_id in October_2023 (11/13)\n",
      "Normalized daily_order_trip_id in October_2024 (12/13)\n",
      "Normalized daily_order_trip_id in September_2024 (13/13)\n"
     ]
    }
   ],
   "source": [
    "# daily_order_trip_id normalization\n",
    "def daily_order_trip_normalization(df):\n",
    "    df = df.to_pandas_df()\n",
    "    # Convert 'Business Day' to datetime format if not already\n",
    "    df['Business Day'] = pd.to_datetime(df['Business Day'])\n",
    "    \n",
    "    # Group by 'Business Day', 'Route Name', 'Direction' and then normalize 'daily_order_trip_id'\n",
    "    df['daily_order_trip_normalized'] = df.groupby(['Business Day', 'Route Name', 'Direction'])['daily_order_trip_id'].transform(\n",
    "        lambda x: np.round((x - 1) / (x.max() - 1),3) if x.nunique() > 1 else 0\n",
    "    )\n",
    "\n",
    "\n",
    "    df = vaex.from_pandas(df)\n",
    "    df['Business Day'] = df['Business Day'].astype('datetime64').dt.strftime('%Y-%m-%d')\n",
    "    #df = df.drop('daily_order_trip_id')\n",
    "    return df\n",
    "\n",
    "for i, (key, df) in enumerate(dfs_dict.items()):\n",
    "    dfs_dict[key] = daily_order_trip_normalization(df)\n",
    "    print(f\"Normalized daily_order_trip_id in {key} ({i+1}/13)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 'Opportunity' rows from April_2024 (1/13)\n",
      "Removed 'Opportunity' rows from August_2024 (2/13)\n",
      "Removed 'Opportunity' rows from December_2023 (3/13)\n",
      "Removed 'Opportunity' rows from Feb_2024 (4/13)\n",
      "Removed 'Opportunity' rows from Jan_2024 (5/13)\n",
      "Removed 'Opportunity' rows from July_2024 (6/13)\n",
      "Removed 'Opportunity' rows from June_2024 (7/13)\n",
      "Removed 'Opportunity' rows from March_2024 (8/13)\n",
      "Removed 'Opportunity' rows from May_2024 (9/13)\n",
      "Removed 'Opportunity' rows from November_2023 (10/13)\n",
      "Removed 'Opportunity' rows from October_2023 (11/13)\n",
      "Removed 'Opportunity' rows from October_2024 (12/13)\n",
      "Removed 'Opportunity' rows from September_2024 (13/13)\n"
     ]
    }
   ],
   "source": [
    "# Remove all rows with 'Opportunity' in the 'TripType' column and remove the column afterwards\n",
    "for i, (key, df) in enumerate(dfs_dict.items()):\n",
    "    #print(f\"Rows before removing 'Opportunity' rows from {key}: {len(df):,}\")\n",
    "    \n",
    "    # Remove rows where 'TripType' == 'Opportunity'\n",
    "    df_filtered = df[df.TripType != 'Opportunity']\n",
    "    \n",
    "    # Drop the 'TripType' column\n",
    "    dfs_dict[key] = df_filtered.drop(columns=['TripType'])\n",
    "    #print(f\"Rows after removing 'Opportunity' rows from {key}: {len(dfs_dict[key]):,}\")\n",
    "    print(f\"Removed 'Opportunity' rows from {key} ({i+1}/{len(dfs_dict)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed columns from April_2024 (1/13)\n",
      "Removed columns from August_2024 (2/13)\n",
      "Removed columns from December_2023 (3/13)\n",
      "Removed columns from Feb_2024 (4/13)\n",
      "Removed columns from Jan_2024 (5/13)\n",
      "Removed columns from July_2024 (6/13)\n",
      "Removed columns from June_2024 (7/13)\n",
      "Removed columns from March_2024 (8/13)\n",
      "Removed columns from May_2024 (9/13)\n",
      "Removed columns from November_2023 (10/13)\n",
      "Removed columns from October_2023 (11/13)\n",
      "Removed columns from October_2024 (12/13)\n",
      "Removed columns from September_2024 (13/13)\n"
     ]
    }
   ],
   "source": [
    "# Remove 'Stop Code', 'Stop Name', 'Vehicle', 'expected_arrival_time', '__TripType' columns\n",
    "\n",
    "for i, (key, df) in enumerate(dfs_dict.items()):\n",
    "    # Drop the columns\n",
    "    dfs_dict[key] = df.drop(columns=['Stop Code', 'Stop Name', 'Vehicle', 'expected_arrival_time', 'Passengers Transfer'])\n",
    "    print(f\"Removed columns from {key} ({i+1}/{len(dfs_dict)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split direction column in April_2024 (1/13)\n",
      "Split direction column in August_2024 (2/13)\n",
      "Split direction column in December_2023 (3/13)\n",
      "Split direction column in Feb_2024 (4/13)\n",
      "Split direction column in Jan_2024 (5/13)\n",
      "Split direction column in July_2024 (6/13)\n",
      "Split direction column in June_2024 (7/13)\n",
      "Split direction column in March_2024 (8/13)\n",
      "Split direction column in May_2024 (9/13)\n",
      "Split direction column in November_2023 (10/13)\n",
      "Split direction column in October_2023 (11/13)\n",
      "Split direction column in October_2024 (12/13)\n",
      "Split direction column in September_2024 (13/13)\n"
     ]
    }
   ],
   "source": [
    "# Split direction column into two columns.\n",
    "def direction_splitting(df):\n",
    "    # East = 1, West = -1, other = 0\n",
    "    df['Horizontal Direction'] = (\n",
    "        (df.Direction == 'E') * 1 +\n",
    "        (df.Direction == 'W') * -1\n",
    "    )\n",
    "    \n",
    "    # North = 1, South = -1, other = 0\n",
    "    df['Vertical Direction'] = (\n",
    "        (df.Direction == 'N') * 1 +\n",
    "        (df.Direction == 'S') * -1\n",
    "    )\n",
    "    #df = df.drop(columns=[\"Direction\"])\n",
    "    return df\n",
    "\n",
    "for i, (key, df) in enumerate(dfs_dict.items()):\n",
    "    dfs_dict[key] = direction_splitting(df)\n",
    "    print(f\"Split direction column in {key} ({i+1}/{len(dfs_dict)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed datetime features in April_2024 (1/13)\n",
      "Processed datetime features in August_2024 (2/13)\n",
      "Processed datetime features in December_2023 (3/13)\n",
      "Processed datetime features in Feb_2024 (4/13)\n",
      "Processed datetime features in Jan_2024 (5/13)\n",
      "Processed datetime features in July_2024 (6/13)\n",
      "Processed datetime features in June_2024 (7/13)\n",
      "Processed datetime features in March_2024 (8/13)\n",
      "Processed datetime features in May_2024 (9/13)\n",
      "Processed datetime features in November_2023 (10/13)\n",
      "Processed datetime features in October_2023 (11/13)\n",
      "Processed datetime features in October_2024 (12/13)\n",
      "Processed datetime features in September_2024 (13/13)\n"
     ]
    }
   ],
   "source": [
    "# Cyclical encoding for month\n",
    "def process_datetime_features(df):\n",
    "    df = df.to_pandas_df()\n",
    "\n",
    "    # Ensure the time column is in datetime format\n",
    "    df['ActualTime'] = pd.to_datetime(df['ActualTime'])\n",
    "    \n",
    "    # Extract hour from the time column\n",
    "    df['hour'] = df['ActualTime'].dt.hour\n",
    "\n",
    "    # Apply cyclical encoding for hour, create a hour_sin and hour_cos column\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24).round(6).apply(lambda x: 0.0 if x == -0.0 else x)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24).round(6).apply(lambda x: 0.0 if x == -0.0 else x)\n",
    "\n",
    "    # Remove hour column\n",
    "    df = df.drop(columns=['hour'])\n",
    "\n",
    "    # Apply cyclical encoding for minute, create a minute_sin and minute_cos column\n",
    "    df['minute_sin'] = np.sin(2 * np.pi * df['ActualTime'].dt.minute / 60).round(6).apply(lambda x: 0.0 if x == -0.0 else x)\n",
    "    df['minute_cos'] = np.cos(2 * np.pi * df['ActualTime'].dt.minute / 60).round(6).apply(lambda x: 0.0 if x == -0.0 else x)\n",
    "\n",
    "\n",
    "    # Ensure the date column is in datetime format\n",
    "    df['Business Day'] = pd.to_datetime(df['Business Day'])\n",
    "\n",
    "\n",
    "    # Extract day of the month and normalized it\n",
    "    df['day_normalized'] = (df['Business Day'].dt.day / df.apply(\n",
    "    lambda row: 29 if (row['Business Day'].month == 2 and \n",
    "                       ((row['Business Day'].year % 4 == 0 and row['Business Day'].year % 100 != 0) or \n",
    "                        (row['Business Day'].year % 400 == 0))) \n",
    "    else {1: 31, 3: 31, 4: 30, 5: 31, 6: 30, 7: 31, 8: 31, 9: 30, 10: 31, 11: 30, 12: 31}.get(row['Business Day'].month, 30), \n",
    "    axis=1\n",
    "    )).round(3)\n",
    "    \n",
    "    # Apply cyclical encoding for day of the month, create a day_sin and day_cos column\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_normalized']).round(6).apply(lambda x: 0.0 if x == -0.0 else x)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_normalized']).round(6).apply(lambda x: 0.0 if x == -0.0 else x)\n",
    "\n",
    "    # Remove day_normalized column\n",
    "    df = df.drop(columns=['day_normalized'])\n",
    "\n",
    "    # Apply cyclical encoding for month, create a month_sin and month_cos column\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['Business Day'].dt.month / 12).round(6).apply(lambda x: 0.0 if x == -0.0 else x)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['Business Day'].dt.month / 12).round(6).apply(lambda x: 0.0 if x == -0.0 else x)\n",
    "\n",
    "    # Remove month column\n",
    "    df = df.drop(columns=['month', 'Start Trip Time'])\n",
    "\n",
    "    # Calculate weekday number (Sunday = 1, Monday = 2, ..., Saturday = 7)\n",
    "    df['weekday_num'] = df['Business Day'].dt.dayofweek + 1\n",
    "\n",
    "\n",
    "    # Convert back to Vaex DataFrame\n",
    "    df = vaex.from_pandas(df)\n",
    "\n",
    "    # Remove the Business Day and ActualTime columns\n",
    "    #df = df.drop(columns=['Business Day', 'ActualTime'])\n",
    "    df = df.drop(columns=['ActualTime'])\n",
    "\n",
    "    # Apply Cyclical Encoding for Weekdays\n",
    "    df['weekday_sin'] = np.sin(2 * np.pi * df['weekday_num'] / 7)\n",
    "    df['weekday_cos'] = np.cos(2 * np.pi * df['weekday_num'] / 7)\n",
    "\n",
    "    # Round small floating-point values and ensure -0.0 becomes 0.0\n",
    "    df['weekday_sin'] = df['weekday_sin'].round(6).apply(lambda x: 0.0 if x == -0.0 else x)\n",
    "    df['weekday_cos'] = df['weekday_cos'].round(6).apply(lambda x: 0.0 if x == -0.0 else x)\n",
    "\n",
    "    # Remove the weekday_num column\n",
    "    df = df.drop(columns=['weekday_num'])\n",
    "    #df['ActualTime'] = df['ActualTime'].astype('datetime64').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['Business Day'] = df['Business Day'].astype('datetime64').dt.strftime('%Y-%m-%d')\n",
    "    return df\n",
    "\n",
    "for i, (key, df) in enumerate(dfs_dict.items()):\n",
    "    dfs_dict[key] = process_datetime_features(df)\n",
    "    print(f\"Processed datetime features in {key} ({i+1}/{len(dfs_dict)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing coordinate normalization in April_2024 (1/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in August_2024 (2/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in December_2023 (3/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in Feb_2024 (4/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in Jan_2024 (5/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in July_2024 (6/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in June_2024 (7/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in March_2024 (8/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in May_2024 (9/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in November_2023 (10/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in October_2023 (11/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in October_2024 (12/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n",
      "Processing coordinate normalization in September_2024 (13/13)\n",
      "Lat Min: 42.042087, Lat Max: 42.34243354, Lon Min: -83.114924, Lon Max: -82.606767\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stop Lat and Stop Lon Normalization\n",
    "# Dictionary to store the min and max values for each column\n",
    "coordinate_mapping = {}\n",
    "\n",
    "def coordinate_normalization(df):\n",
    "    lat_min = df['stop_lat'].min()\n",
    "    lat_max = df['stop_lat'].max()\n",
    "    lon_min = df['stop_lon'].min()\n",
    "    lon_max = df['stop_lon'].max()\n",
    "    print(f'Lat Min: {lat_min}, Lat Max: {lat_max}, Lon Min: {lon_min}, Lon Max: {lon_max}\\n\\n')\n",
    "\n",
    "    # Normalize the columns (Min-Max Scaling where min -> 0 and max -> 1)\n",
    "    df['stop_lat_norm'] = (df['stop_lat'] - lat_min) / (lat_max - lat_min)\n",
    "    df['stop_lon_norm'] = (df['stop_lon'] - lon_min) / (lon_max - lon_min)\n",
    "\n",
    "    # Store the min and max values in a dictionary\n",
    "    normalization_params = {\n",
    "        \"stop_lat\": [lat_min, lat_max],\n",
    "        \"stop_lon\": [lon_min, lon_max]\n",
    "    }\n",
    "    df = df.drop(columns=['stop_lat', 'stop_lon'])\n",
    "    return df, normalization_params\n",
    "\n",
    "for i, (key, df) in enumerate(dfs_dict.items()):\n",
    "    print(f\"Processing coordinate normalization in {key} ({i+1}/13)\")\n",
    "    dfs_dict[key], coordinate_mapping[key] = coordinate_normalization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Overcrowded status column in April_2024 (1/13)\n",
      "Created Overcrowded status column in August_2024 (2/13)\n",
      "Created Overcrowded status column in December_2023 (3/13)\n",
      "Created Overcrowded status column in Feb_2024 (4/13)\n",
      "Created Overcrowded status column in Jan_2024 (5/13)\n",
      "Created Overcrowded status column in July_2024 (6/13)\n",
      "Created Overcrowded status column in June_2024 (7/13)\n",
      "Created Overcrowded status column in March_2024 (8/13)\n",
      "Created Overcrowded status column in May_2024 (9/13)\n",
      "Created Overcrowded status column in November_2023 (10/13)\n",
      "Created Overcrowded status column in October_2023 (11/13)\n",
      "Created Overcrowded status column in October_2024 (12/13)\n",
      "Created Overcrowded status column in September_2024 (13/13)\n"
     ]
    }
   ],
   "source": [
    "# Create overcrowded status column (35 or less = 0, more than 35 = 1) from Actual Bus Occupancy column\n",
    "\n",
    "def overcrowded_status(df):\n",
    "    df['Overcrowded'] = (df['Actual Bus Occupancy'] > 35) * 1\n",
    "    return df\n",
    "\n",
    "for i, (key, df) in enumerate(dfs_dict.items()):\n",
    "    dfs_dict[key] = overcrowded_status(df)\n",
    "    print(f\"Created Overcrowded status column in {key} ({i+1}/13)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Split (Training, Validation and Testing)\n",
    "* First for September 2024\n",
    "* Take all Overcrowded Trips (Group by Route Name, Direction, daily_order_trip_id, Business day)\n",
    "* Count all trips with at least 1 overcrowded row\n",
    "* And then count all trip with none overcrowded rows\n",
    "* Take a sample of all the rows for the overcrowded trips and then take another sample of all  none overcrowded trips of the same size so the final dataset has 50% overcrowded trips and 50% no-overcrowded trips\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_id</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Route Name</th>\n",
       "      <th>Passengers In</th>\n",
       "      <th>Passengers Out</th>\n",
       "      <th>Actual Bus Occupancy</th>\n",
       "      <th>Business Day</th>\n",
       "      <th>daily_order_trip_id</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_sequence_normalized</th>\n",
       "      <th>...</th>\n",
       "      <th>minute_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>weekday_sin</th>\n",
       "      <th>weekday_cos</th>\n",
       "      <th>stop_lat_norm</th>\n",
       "      <th>stop_lon_norm</th>\n",
       "      <th>Overcrowded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_518_E_1</td>\n",
       "      <td>E</td>\n",
       "      <td>518</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2024-09-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207912</td>\n",
       "      <td>0.205863</td>\n",
       "      <td>0.978581</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.679239</td>\n",
       "      <td>0.191049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    group_id Direction Route Name  Passengers In  Passengers Out  \\\n",
       "0  1_518_E_1         E        518              8               0   \n",
       "\n",
       "   Actual Bus Occupancy Business Day  daily_order_trip_id  stop_sequence  \\\n",
       "0                     8   2024-09-01                    1              1   \n",
       "\n",
       "   stop_sequence_normalized  ...  minute_cos   day_sin   day_cos  month_sin  \\\n",
       "0                       0.0  ...    0.207912  0.205863  0.978581       -1.0   \n",
       "\n",
       "   month_cos  weekday_sin  weekday_cos  stop_lat_norm  stop_lon_norm  \\\n",
       "0        0.0          0.0          1.0       0.679239       0.191049   \n",
       "\n",
       "   Overcrowded  \n",
       "0            0  \n",
       "\n",
       "[1 rows x 26 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert th vaex dataframe to pandas dataframe\n",
    "df = dfs_dict['September_2024'].copy()\n",
    "df = df.to_pandas_df()\n",
    "\n",
    "# Group by 'Business Day', 'Route Name', 'Direction' and 'daily_order_trip_id' and make a id for each group\n",
    "# by combining the elements of the group, similar to adding strings together.\n",
    "\n",
    "df['group_id'] = pd.to_datetime(df['Business Day']).dt.day.astype(str) + '_' + df['Route Name'].astype(str) + '_' + df['Direction'].astype(str) + '_' + df['daily_order_trip_id'].astype(str)\n",
    "\n",
    "# Show groud_id column at the beginning of the dataframe\n",
    "cols = list(df.columns)\n",
    "cols.remove('group_id')\n",
    "cols = ['group_id'] + cols\n",
    "df = df[cols]\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups with at least one overcrowded bus: 2,243\n",
      "Number of groups with no overcrowded bus: 18,616\n",
      "Total number of unique groups: 20,859\n",
      "Sampled non-overcrowded groups: 2,243\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify groups with at least one overcrowded bus\n",
    "overcrowded_groups = set(df[df['Overcrowded'] == 1]['group_id'].unique())\n",
    "\n",
    "# Step 2: Identify groups that NEVER had an overcrowded bus\n",
    "all_groups = set(df['group_id'].unique())\n",
    "non_overcrowded_groups = all_groups - overcrowded_groups  # Set difference ensures no overlap\n",
    "\n",
    "# Step 3: Print counts to verify separation\n",
    "print(f\"Number of groups with at least one overcrowded bus: {len(overcrowded_groups):,}\")\n",
    "print(f\"Number of groups with no overcrowded bus: {len(non_overcrowded_groups):,}\")\n",
    "print(f\"Total number of unique groups: {len(all_groups):,}\")\n",
    "assert len(overcrowded_groups) + len(non_overcrowded_groups) == len(all_groups), \"Error: Group splitting is incorrect!\"\n",
    "\n",
    "# Step 4: Create a radnom sample from the non_overcrowded_groups that has the same size as overcrowded_groups\n",
    "random.seed(42)  # Optional: For reproducibility\n",
    "sampled_non_overcrowded = random.sample(non_overcrowded_groups, len(overcrowded_groups))\n",
    "\n",
    "# Print sample size to verify\n",
    "print(f\"Sampled non-overcrowded groups: {len(sampled_non_overcrowded):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ No group_id duplication across Train, Validation, and Test sets!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split Overcrowded and Non-Overcrowded Groups Separately\n",
    "# Split overcrowded groups: 70% train, 20% validation, 10% test\n",
    "train_overcrowded, temp_overcrowded = train_test_split(list(overcrowded_groups), test_size=0.3, random_state=42)\n",
    "val_overcrowded, test_overcrowded = train_test_split(temp_overcrowded, test_size=1/3, random_state=42)\n",
    "\n",
    "# Split non-overcrowded groups: 70% train, 20% validation, 10% test\n",
    "train_non_overcrowded, temp_non_overcrowded = train_test_split(list(sampled_non_overcrowded), test_size=0.3, random_state=42)\n",
    "val_non_overcrowded, test_non_overcrowded = train_test_split(temp_non_overcrowded, test_size=1/3, random_state=42)\n",
    "\n",
    "# Combine Train, Validation, and Test Group IDs\n",
    "train_group_ids = set(train_overcrowded) | set(train_non_overcrowded)\n",
    "val_group_ids = set(val_overcrowded) | set(val_non_overcrowded)\n",
    "test_group_ids = set(test_overcrowded) | set(test_non_overcrowded)\n",
    "\n",
    "# Ensure No Overlap Between Splits\n",
    "assert len(train_group_ids & val_group_ids) == 0, \"Train and Validation sets overlap!\"\n",
    "assert len(train_group_ids & test_group_ids) == 0, \"Train and Test sets overlap!\"\n",
    "assert len(val_group_ids & test_group_ids) == 0, \"Validation and Test sets overlap!\"\n",
    "\n",
    "print(\"✅ No group_id duplication across Train, Validation, and Test sets!\")\n",
    "\n",
    "# Step 7: Create Final DataFrames\n",
    "train_df = df[df['group_id'].isin(train_group_ids)]\n",
    "val_df = df[df['group_id'].isin(val_group_ids)]\n",
    "test_df = df[df['group_id'].isin(test_group_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove group_id, Business Day, Direction, daily_order_trip_id, stop_sequence columns\n",
    "train_df = train_df.drop(columns=['group_id', 'Business Day', 'Direction', 'daily_order_trip_id', 'stop_sequence'])\n",
    "val_df = val_df.drop(columns=['group_id', 'Business Day', 'Direction', 'daily_order_trip_id', 'stop_sequence'])\n",
    "test_df = test_df.drop(columns=['group_id', 'Business Day', 'Direction', 'daily_order_trip_id', 'stop_sequence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are all target encoding values unique? True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Route Name\n",
       "10        0.012724\n",
       "115       0.310453\n",
       "14        0.000007\n",
       "1A        0.138168\n",
       "1C        0.112792\n",
       "2         0.065252\n",
       "25        0.000001\n",
       "3         0.047447\n",
       "4         0.008533\n",
       "418       0.003650\n",
       "42        0.000000\n",
       "518       0.159385\n",
       "6         0.155231\n",
       "605       0.000002\n",
       "7         0.045788\n",
       "8         0.054954\n",
       "TUNNEL    0.033642\n",
       "Name: Overcrowded, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Target Encode the trainig set and apply the same encoding to the validation and test sets\n",
    "# Compute mean overcrowding per Route Name\n",
    "target_encodings = train_df.groupby('Route Name')['Overcrowded'].mean()\n",
    "\n",
    "# Add random noise (different per Route Name)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "random_noise = np.round(np.random.uniform(0, 0.00001, size=len(target_encodings)),6)\n",
    "target_encodings += random_noise\n",
    "\n",
    "train_df['Route Name'] = train_df['Route Name'].map(target_encodings)\n",
    "val_df['Route Name'] = val_df['Route Name'].map(target_encodings)\n",
    "test_df['Route Name'] = test_df['Route Name'].map(target_encodings)\n",
    "\n",
    "# Check if all values are unique\n",
    "is_unique = target_encodings.nunique() == len(target_encodings)\n",
    "\n",
    "print(f\"Are all target encoding values unique? {is_unique}\")\n",
    "\n",
    "\n",
    "target_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder to store the cleaned datasets named 'split_cleaned_dataset'\n",
    "if not os.path.exists('split_cleaned_dataset'):\n",
    "    os.makedirs('split_cleaned_dataset')\n",
    "\n",
    "# Save the cleaned datasets to CSV files\n",
    "train_df.to_csv('split_cleaned_dataset/train_september.csv', index=False)\n",
    "val_df.to_csv('split_cleaned_dataset/val_september.csv', index=False)\n",
    "test_df.to_csv('split_cleaned_dataset/test_september.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vaex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
